read image mean succeeded
196608
1	3	256	256
110.177	110.459

Input blob size:
10	3	227	227

#Layers: 23

[1] layer name: conv1, type: Convolution
#Blobs: 2
96	3	11	11
-0.00121359	0.00323653
96	1	1	1
-0.504212	-0.188764

[2] layer name: relu1, type: ReLU
#Blobs: 0

[3] layer name: pool1, type: Pooling
#Blobs: 0

[4] layer name: norm1, type: LRN
#Blobs: 0

[5] layer name: conv2, type: Convolution
#Blobs: 2
256	48	5	5
-0.0111258	0.0218567
256	1	1	1
0.982103	0.994344

[6] layer name: relu2, type: ReLU
#Blobs: 0

[7] layer name: pool2, type: Pooling
#Blobs: 0

[8] layer name: norm2, type: LRN
#Blobs: 0

[9] layer name: conv3, type: Convolution
#Blobs: 2
384	256	3	3
-0.000527019	0.00534925
384	1	1	1
-0.000547366	-0.00652369

[10] layer name: relu3, type: ReLU
#Blobs: 0

[11] layer name: conv4, type: Convolution
#Blobs: 2
384	192	3	3
0.00378311	0.000618855
384	1	1	1
0.884884	1.02955

[12] layer name: relu4, type: ReLU
#Blobs: 0

[13] layer name: conv5, type: Convolution
#Blobs: 2
256	192	3	3
-0.0134502	-0.0174268
256	1	1	1
0.982957	1.04293

[14] layer name: relu5, type: ReLU
#Blobs: 0

[15] layer name: pool5, type: Pooling
#Blobs: 0

[16] layer name: fc6, type: InnerProduct
#Blobs: 2
4096	9216	1	1
0.00639847	0.00915686
4096	1	1	1
0.983698	1.00962

[17] layer name: relu6, type: ReLU
#Blobs: 0

[18] layer name: drop6, type: Dropout
#Blobs: 0

[19] layer name: fc7, type: InnerProduct
#Blobs: 2
4096	4096	1	1
0.0125212	-0.0134137
4096	1	1	1
1.09399	0.999266

[20] layer name: relu7, type: ReLU
#Blobs: 0

[21] layer name: drop7, type: Dropout
#Blobs: 0

[22] layer name: fc8, type: InnerProduct
#Blobs: 2
1000	4096	1	1
0.000160601	-0.010393
1000	1	1	1
-0.196648	-0.100678

[23] layer name: prob, type: Softmax
#Blobs: 0

weights and bias from layer: conv1
-0.00121359	0.00323653
-0.504212	-0.188764

new weights and bias from layer: conv1
1.1111	2.2222

#Features: 2904000
0	0
END
