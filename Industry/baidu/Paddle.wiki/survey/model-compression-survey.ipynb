{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model compression survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "DNN models are computationlly expensive and memory intensive, \n",
    "the model compression way is used to accelerate the deep network without significantly decreasing the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model compression approaches can be classified into four categories:\n",
    "    \n",
    "- parameter pruning and sharing\n",
    "    - reducing redundant parameters which are not sensitive to the performance\n",
    "- low-rank factorization\n",
    "    - using matrix/tensor decomposition to estimate the informative parameters\n",
    "- transferred/compact convolutional filters\n",
    "    - designing special structural convolutional filters to save parameters\n",
    "- knowledge distillation\n",
    "    - training a compact neural network with distilled knowledge of a large model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will dive into the details of these four methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter pruning and sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technology can be further classified into three categories:\n",
    "\n",
    "- model quntization and binarization\n",
    "- parameter sharing \n",
    "- structural matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization and Binarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*reference: Improving the speed of neural networks on CPUs*\n",
    "\n",
    "#### Floating-point implementation (competitive with Eigen in small matrice mulplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loop unrolling and parallel accumulators gets 26% speed improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix multipication is as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c++\n",
    "// two matrice\n",
    "// matrix A's shape is (m, n)\n",
    "float **A;\n",
    "// matrix B's shape is (n, t) \n",
    "float **B;\n",
    "// A multiply B will get a new matrix of shape (m, t)\n",
    "float **C;\n",
    "for (int i = 0; i < m; i++) {\n",
    "    for (int j = 0; j < n; j++) {\n",
    "        C[i][j] = 0.;\n",
    "        for (int k = 0; k < t; k++) { // NOTE this for-loop takes much time to check termination\n",
    "            c[i][j] += A[i][k] * B[j][k];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To reduce the overhead of checking for-loop termination, one can unroll the computation by accumulating elements at a time.**\n",
    "\n",
    "```c++\n",
    "auto& c = C[i][j];\n",
    "auto* a = A[i];\n",
    "auto* b = B[j];\n",
    "C += a[k]*b[k] + a[k+1]*b[k+1] + a[k+2]*b[k+2] + a[k+3]*b[k+3];\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a second technique is to **use multiple accumulators in parallel, \n",
    "the compiler might optimize it in pipelining operations.** for example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c++\n",
    "c0 += a[i] * b[i];\n",
    "c1 += a[i+1] * b[i+1];\n",
    "c2 += a[i+2] * b[i+2];\n",
    "c3 += a[i+3] * b[i+3];\n",
    "c = c0 + c1 + c2 + c3;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SIMD to make low-level parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Intel and AMD CPUs of the x86 family, the SIMD instructions typically operate on 16 bytes worth of data at a time, that is 2 doubles, 4 floats, 8 shorts or 16 bytes at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"./images/simd.png/\" style=\"width: 600px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code \n",
    "\n",
    "```c++\n",
    "c += a[k]*b[k] + a[k+1]*b[k+1] + a[k+2]*b[k+2] + a[k+3]*b[k+3];\n",
    "```\n",
    "which is a sum of 4 floats can be rewritten to\n",
    "\n",
    "```c++\n",
    "#include <mmintrin.h>\n",
    "__m128 c = _mm_add_ps(a, b);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SIMD and SSE2  to perform multiply-and-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simplified example accumulating the scalar product of `__m128 *a` and `__m128 *b` to `__m128 sum`\n",
    "\n",
    "```c++\n",
    "// c[0] = a[0]*b[0], ..., c[3] = a[3]*b[3]\n",
    "__m128 c = _mm_mul_ps(*a, *b);\n",
    "// sum[0] += c[0], ..., sum[3] += c[3]\n",
    "sum = _mm_add_ps(c, sum);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed-point implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
